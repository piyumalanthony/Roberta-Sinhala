{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe8eab8e"
      },
      "source": [
        "# !pip3 install torch"
      ],
      "id": "fe8eab8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ab52939"
      },
      "source": [
        "# !pip3 install tqdm"
      ],
      "id": "3ab52939",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58c09c24"
      },
      "source": [
        "# !pip3 install transformers"
      ],
      "id": "58c09c24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6fb4244",
        "outputId": "5cf99235-e765-4c64-876f-d8187f7ce5b4"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.__version__"
      ],
      "id": "b6fb4244",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.9.1+cu102'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50b7b142"
      },
      "source": [
        "from pathlib import Path\n",
        "paths = [str(x) for x in Path('/userdirs/piyumal/roberta_sinhala/content/sinhala-dataset-creation/datasets/tokenized/').glob('**/*.txt')]"
      ],
      "id": "50b7b142",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a05d11d1",
        "outputId": "4795a29e-090b-4c8b-b34c-df17a039bb60"
      },
      "source": [
        "len(paths)"
      ],
      "id": "a05d11d1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ec56499"
      },
      "source": [
        "# from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# tokenizer = ByteLevelBPETokenizer()"
      ],
      "id": "8ec56499",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "720336f5"
      },
      "source": [
        "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2,\n",
        "#                 special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
      ],
      "id": "720336f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7b5e5ab"
      },
      "source": [
        "# import os\n",
        "\n",
        "# os.mkdir('./Roberta_tokenizer')\n",
        "\n",
        "# tokenizer.save_model('Roberta_tokenizer')\n",
        "# tokenizer.save(\"./Roberta_tokenizer/config.json\") \n",
        "# tokenizer.save(\"./Roberta_tokenizer/tokenizer.json\") "
      ],
      "id": "c7b5e5ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbb05b2f",
        "outputId": "f72183c4-55e8-4c25-abc4-dccbb4538bad"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('Roberta_tokenizer', max_len=512)"
      ],
      "id": "bbb05b2f",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce8e3e78"
      },
      "source": [
        "tokens = tokenizer(\"‡∂ë‡∑Ä‡∑í‡∂ß ‡∂Ø‡∑Ä‡∑É‡∑í‡∂±‡∑ä ‡∂Ø‡∑Ä‡∑É ‡∂Ö‡∑Ä‡∂≠‡∑ê‡∂±‡∑ä‡∑Ä‡∑ñ‡∑Ä‡∂±‡∑ä ‡∑É‡∑í‡∂ß‡∑í‡∂± ‡∂ö‡∂≥‡∑Ä‡∑î‡∂ª‡∑î‡∑Ä‡∂Ω‡∂ß ‡∂∏‡∑ô‡∂∏ ‡∑É‡∑û‡∂õ‡∑ä‚Äç‡∂∫ ‡∂ö‡∂´‡∑ä‡∂©‡∑è‡∂∫‡∂∏‡∑ä ‡∂∫‡∑Ä‡∂±‡∑Ä‡∑è\")"
      ],
      "id": "ce8e3e78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "157ec3a1",
        "outputId": "e2c8d4d6-e78d-49d8-eb13-f60c995da309"
      },
      "source": [
        "print(tokens)"
      ],
      "id": "157ec3a1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 1056, 265, 280, 599, 265, 266, 264, 599, 1702, 284, 266, 264, 267, 320, 329, 264, 286, 265, 280, 265, 266, 1366, 272, 270, 272, 574, 288, 276, 273, 286, 501, 490, 294, 269, 586, 264, 304, 268, 415, 264, 3476, 268, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9286b0a1"
      },
      "source": [
        "with open('Sinhala_all_data.txt', 'r', encoding='utf-8') as fp:\n",
        "    lines = fp.read().split('\\n')"
      ],
      "id": "9286b0a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb705f74",
        "outputId": "4885ec3b-50a2-4dc4-848f-16e898630b65"
      },
      "source": [
        "lines[:100]"
      ],
      "id": "fb705f74",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '‡∂Ω‡∑è‡∂Ω‡∑ä ‡∂ª‡∑ñ‡∂¥‡∂≠‡∑î‡∂Ç‡∂ú ‡∂ª‡∂≠‡∑í‡∂•‡∑è‡∑Ä‡∂ö‡∑ä ‡∂¥‡∂≠‡∑ä‡∂≠‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ã‡∂≠‡∑ä‡∑É‡∑Ñ ‡∂Ø‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∂Ω‡∑í‡∂∫‡∂±‡∂¥‡∂≠‡∑í‡∂ª‡∂´ ‡∂ª‡∂≠‡∑í‡∂§‡∑è‡∑Ä‡∂ö‡∑ä ‡∂¥‡∂≠‡∑ä‡∂≠‡∑î ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑Ñ‡∂≠ ‡∂∏‡∑è‡∂Ω‡∂∫‡∂ß ‡∑Ä‡∑í‡∑É‡∑í ‡∂ö‡∂Ω‡∑ö‡∂∫',\n",
              " '‡∂¢‡∂±‡∂≠‡∑è‡∑Ä ‡∂¢‡∂±‡∑ä‡∂Ø‡∂∫‡∑ô‡∂±‡∑ä ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∂ö‡∑ä‡∑Ç‡∑ö‡∂¥ ‡∂ö‡∑Ö ‡∂Ö‡∂∫ ‡∂Ø ‡∂Ø‡∑ê‡∂±‡∑ä ‡∂í ‡∂Ü‡∂´‡∑ä‡∂©‡∑î‡∑Ä‡∑ö ‡∂±‡∑í‡∂Ω‡∂≠‡∂Ω ‡∂Ø‡∂ª‡∂≠‡∑í',\n",
              " '‡∂¢‡∂∫‡∑Ä‡∑í‡∂Ω‡∑è‡∂Ω‡∑ä ‡∑Ä‡∑í‡∂Ω‡∑ö‡∂ú‡∑ú‡∂©‡∂ú‡∑ö ‡∂±‡∑ú‡∂¥‡∑Ö ‡∂Ω‡∑í‡∂¥‡∑í ‡∑É‡∑î‡∂±‡∑í‡∂Ω‡∑ä ‡∂∏‡∑í‡∑Ñ‡∑í‡∂≥‡∑î‡∂ö‡∑î‡∂Ω ‡∂¥‡∑í',\n",
              " '‡∂Ö‡∂∞‡∑í‡∂ª‡∑è‡∂¢‡∑ä\\u200d‡∂∫‡∂∫‡∂ß ‡∂ú‡∑ê‡∂≠‡∑í ‡∑Ä‡∑ô‡∂≠‡∑í',\n",
              " '‡∂¥‡∑ä\\u200d‡∂ª‡∑Å‡∑ä‡∂±‡∑ô‡∂ö‡∂ß ‡∂ö‡∑í‡∂∫‡∂Ω ‡∂Ω‡∑ú‡∂ö‡∑î‡∑Ä‡∂ß‡∂∏ ‡∂Ü‡∑Ä‡∑ô ‡∂ë‡∂∫‡∑è ‡∂¥‡∑í‡∂ß ‡∂ª‡∂ß ‡∂â‡∂±‡∑ä‡∂± ‡∂ë‡∂∫‡∑è‡∂ú‡∑ô ‡∂∫‡∑è‡∂Ω‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä ‡∂ö‡∑í‡∂∫‡∂Ω ‡∂ö‡∑í‡∂∫‡∂¥‡∑î ‡∂ú‡∂ª‡∑ä‡∂Ω‡∑ä ‡∂ö‡∑ô‡∂±‡∑ô‡∂ö‡∑ä ‡∂ë‡∂ö‡∑ä‡∂ö ‡∂ï‡∂±‡∑ô‡∑Ä‡∂ß ‡∑Ä‡∂©‡∑è ‡∂ö‡∂≠‡∑è‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂¥‡∂ß‡∂Ç ‡∂ú‡∂≠‡∑ä‡∂≠ ‡∂ë‡∂ö ‡∂≠‡∂∏‡∂∫‡∑í',\n",
              " '‡∂∏‡∂∏‡∂≠‡∑ä ‡∂ë‡∑Ñ‡∑ô‡∂∏ ‡∑Ä‡∑ô‡∂Ω‡∑è ‡∑Ñ‡∑í‡∂ß‡∑í‡∂∫ ‡∂±‡∑í‡∑É‡∑è ‡∂∏‡∂∏ ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂Ø‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∂í‡∂Ø‡∑ö',\n",
              " '‡∂î‡∂∫‡∑è‡∂Ω ‡∂ö‡∂ª‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂± ‡∂ë‡∂ö‡∑ö ‡∂ë‡∂ö ‡∑Ñ‡∂ª‡∑í‡∂∫‡∂ß‡∂∏ ‡∂∂‡∂Ω‡∑è‡∂ú‡∂±‡∑ä‡∂± ‡∂±‡∂∏‡∑ä ‡∂ë‡∂ö‡∑ö ‡∂ö‡∑í‡∂∫‡∂± ‡∂≠‡∑ê‡∂± ‡∂∂‡∂Ω‡∂±‡∑ä‡∂±',\n",
              " '‡∂±‡∑ì‡∂≠‡∑ä\\u200d‡∂∫‡∂±‡∑î‡∂ö‡∑ñ‡∂Ω ‡∑Ä ‡∂ª‡∂ß ‡∂∫‡∂±‡∑ä‡∂±',\n",
              " '‡∂ë‡∂∫ ‡∑Å‡∑ä\\u200d‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è‡∑Ä‡∑è‡∑É‡∑ì‡∂±‡∑ä‡∂ß ‡∂±‡∑ê‡∑Ä‡∑î‡∂∏‡∑ä ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∑Ä‡∂±‡∑ä‡∂±‡∑ö ‡∂ë‡∂∫ ‡∂∏‡∑î‡∑Ñ‡∑î‡∂Ø ‡∂ú‡∑ú‡∂©‡∂ö‡∂ª ‡∂â‡∂Ø‡∑í‡∂ö‡∑ô‡∂ª‡∑î‡∂´‡∑î ‡∂±‡∂ú‡∂ª‡∂∫‡∂ö‡∑ä ‡∂±‡∑í‡∑É‡∑è‡∑Ä‡∑ô‡∂±‡∑í',\n",
              " '‡∂í ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥ ‡∂±‡∑í‡∑Ä‡∑ö‡∂Ø‡∂±‡∂Ø ‡∂±‡∑í‡∂ö‡∑î‡∂≠‡∑ä ‡∂ö‡∂ª‡∂∫‡∑í',\n",
              " '‡∂Ö‡∂¥ ‡∂∫‡∑î‡∂ö‡∑ä‡∂≠‡∑í‡∂∫ ‡∂¥‡∑î‡∑Ä‡∂≠‡∑ä‡∂¥‡∂≠ ‡∂¥‡∑Ä‡∂≠‡∑ä‡∑Ä‡∑è‡∂ú‡∑ô‡∂± ‡∂ú‡∑í‡∂∫ ‡∂ö‡∑è‡∂Ω‡∂∫‡∑ö ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω ‡∂∂‡∑û‡∂Ø‡∑ä‡∂∞‡∑è‡∂ú‡∂∏‡∑ö ‡∂∏‡∑î‡∂Ø‡∑î‡∂±‡∑ä‡∂∏‡∂Ω‡∑ä‡∂ö‡∂© ‡∑Ä‡∂± ‡∂∑‡∑í‡∂ö‡∑ä‡∑Ç‡∑ñ‡∂±‡∑ä ‡∂¥‡∑Ä‡∑è ‡∂ª‡∑Ñ‡∂∏‡∑ô‡∂ª‡∂ß ‡∂Ü‡∑É‡∑è ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∑Å‡∑ä‡∂†‡∑í‡∂≠ ‡∑É‡∑è‡∂ö‡∑ä‡∑Ç‡∑í ‡∂≠‡∑í‡∂∂‡∑î‡∂´‡∑í',\n",
              " '‡∑Ñ‡∑í‡∂ß‡∂¥‡∑î ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂∏‡∑Ñ‡∑í‡∂±‡∑ä‡∂Ø ‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂∏‡∑Ñ‡∂≠‡∑è‡∂ú‡∑ö ‡∂¥‡∑í‡∂∫‡∑è ‡∑Ä‡∑ñ ‡∂©‡∑ì.‡∂í. ‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂∏‡∑Ñ‡∂≠‡∑è‡∂ú‡∑ö ‡∂ú‡∑î‡∂´ ‡∑É‡∂∏‡∂ª‡∑î ‡∑É‡∂ß‡∑Ñ‡∂±‡∂ö‡∑ä ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∑ô‡∑É‡∑ö ‡∂¥‡∑è‡∑É‡∂Ω‡∑ä ‡∑É‡∑í‡∑É‡∑î ‡∑É‡∑í‡∑É‡∑î‡∑Ä‡∑í‡∂∫‡∂±‡∑ä ‡∂ú‡∑ô‡∂±‡∑ä‡∑Ä‡∑è ‡∂≠‡∑í‡∂∂‡∑î‡∂´‡∑í',\n",
              " '‡∑Ä‡∑í‡∂ö‡∂ß ‡∂ª‡∂Ç‡∂ú‡∂± ‡∑Å‡∑í‡∂Ω‡∑ä‡∂¥‡∑ì ‡∂¥‡∑í‡∂±‡∑ä ‡∂¥‡∑ú‡∂±‡∑ä ‡∑É‡∂∏‡∑è‡∑Ä‡∂ú‡∂≠‡∑ä ‡∑Ä‡∑ì‡∂©‡∑í‡∂∫‡∑ù‡∑Ä ‡∂â‡∑Ä‡∂≠‡∑ä ‡∂ö‡∂ª‡∂ú‡∂±‡∑ì',\n",
              " '‡∂≠‡∂ª‡∑Ñ‡∑è ‡∂ú‡∂±‡∑ä‡∂± ‡∂ë‡∂¥‡∑è ‡∂∏‡∂†‡∑ù ‡∂Ω‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ö ‡∂∏‡∑ö ‡∑Ä‡∑í‡∂Ø‡∑í‡∂∫‡∂ß‡∂∏ ‡∂≠‡∂∏‡∑è ‡∑Ä‡∑ô‡∂± ‡∂≠‡∑ê‡∂±‡∑ä ‡∑Ä‡∂Ω‡∂≠‡∑ä ‡∂â‡∑É‡∑ä‡∑É‡∂ª ‡∂Ω‡∑í‡∑Ä‡∑ä‡∑Ä‡∑è ‡∂í‡∑Ä‡∑è‡∂∫‡∑ô‡∂≠‡∑ä',\n",
              " '‡∂ë‡∂ö‡∑ä‡∑É‡∂≠‡∑ä ‡∂¢‡∑è‡∂≠‡∑í‡∂ö ‡∂¥‡∂ö‡∑ä‡∑Ç‡∂∫ ‡∂∫‡∂±‡∑î ‡∂∏‡∑ô‡∂ª‡∂ß ‡∂¥‡∑ä\\u200d‡∂ª‡∂Æ‡∂∏ ‡∂Ø‡∑ö‡∑Å‡∂¥‡∑è‡∂Ω‡∂± ‡∂¥‡∂ö‡∑ä‡∑Ç‡∂∫‡∂∫‡∑í',\n",
              " '‡∂Ö‡∂∏‡∑ä‡∂∂‡∂Ω‡∂Ç‡∂ú‡∑ú‡∂© ‡∂â‡∂©‡∂∏‡∑ä‡∂≠‡∑ú‡∂ß ‡∂¥‡∑ä\\u200d‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö‡∂Ø‡∑ì ‡∂±‡∑í‡∑Ä‡∑É‡∂ö‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂Ö‡∂∑‡∑í‡∂ª‡∑Ñ‡∑É‡∑ä ‡∂Ω‡∑ô‡∑É ‡∂∏‡∑í‡∂∫‡∂ú‡∑í‡∂∫',\n",
              " '‡∂ë‡∑Ä‡∑ô‡∂Ω‡∑ö ‡∂ö‡∂©‡∂¥‡∑î ‡∂¥‡∑ê‡∂´‡∑í ‡∂ö‡∑ú‡∂∏‡∂©‡∑î üòÄ',\n",
              " '‡∂ö‡∑í‡∑É‡∑í‡∂∏ ‡∂∏‡∂±‡∑î‡∑É‡∑ä‡∑É‡∂∫‡∑ú ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑ä ‡∂ú‡∂≠‡∑ä‡∂≠‡∑ú‡∂≠‡∑ä ‡∂í ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑É‡∂ª‡∑ä‡∑Ä ‡∑É‡∂∏‡∑è‡∂± ‡∂±‡∑ë',\n",
              " '‡∑É‡∂∫‡∑í‡∂ß‡∂∏‡∑ä ‡∂±‡∑Ä‡∂≠‡∑ä‡∂≠‡∂±‡∑ä‡∂± ‡∂±‡∑ô‡∂∏‡∑ô‡∂∫‡∑í ‡∂í‡∂ö ‡∂ª‡∂¢‡∂∫‡∂ß ‡∑Ä‡∑è‡∑É‡∑í ‡∂Ø‡∑è‡∂∫‡∂ö ‡∑Ä‡∑ô‡∂± ‡∑Ä‡∑í‡∂Ø‡∑í‡∑Ñ‡∂ß ‡∂ª‡∂¢‡∂∫‡∑ö ‡∑Ä‡∑õ‡∂Ø‡∑ä\\u200d‡∂∫ ‡∑Ä‡∑í‡∂Ø‡∑ä\\u200d‡∂∫‡∑è‡∂Ω‡∑Ä‡∂Ω ‡∂ú‡∑î‡∂´‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∂≠‡∂≠‡∑ä‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂Ø‡∑í‡∂∫‡∑î‡∂´‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂Ö‡∑Ä‡∑Å‡∑ä\\u200d‡∂∫‡∂∫ ‡∑Ä‡∑í‡∂Ø‡∑í‡∑Ñ‡∂ß ‡∂∏‡∑ô‡∑Ñ‡∑ô‡∂∫‡∑Ä‡∂±‡∑ä‡∂± ‡∂ª‡∂¢‡∂∫ ‡∂∏‡∑ê‡∂Ø‡∑í‡∑Ñ‡∂≠‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∑Ñ‡∑ê‡∂ß‡∑í ‡∂∏‡∑ô‡∂≠‡∂± ‡∂Ω‡∑í‡∂∫‡∑ê‡∑Ä‡∑ì ‡∂±‡∑ê‡∂Ø‡∑ä‡∂Ø',\n",
              " '‡∂ö‡∑ú‡∂ª‡∑í‡∂∫‡∑è‡∂±‡∑î ‡∂∑‡∑è‡∑Ç‡∑è ‡∂¥‡∑ä\\u200d‡∂ª‡∑Ä‡∑ì‡∂±‡∂≠‡∑è ‡∑Ä‡∑í‡∂∑‡∑è‡∂ú‡∂∫‡∂ß ‡∑Ä‡∑ê‡∂©‡∑í‡∂∏ ‡∂Ö‡∂∫‡∂Ø‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä ‡∂≠‡∂Ç‡∂ú‡∂Ω‡∑ä‡∂Ω‡∑ô‡∂±‡∑ä',\n",
              " '‡∂ö‡∑ú‡∂ª‡∑í‡∂∫‡∑è ‡∂Ö‡∂ª‡∑ä‡∂∞‡∂Ø‡∑ä‡∑Ä‡∑ì‡∂¥‡∂∫ ‡∂±‡∑ä\\u200d‡∂∫‡∑Ç‡∑ä‡∂ß‡∑í‡∂ö ‡∂Ö‡∑Ä‡∑í‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂≠‡∑ú‡∂ª ‡∂ö‡∂Ω‡∑è‡∂¥‡∂∫‡∂ö‡∑ä ‡∂∂‡∑Ä‡∂ß ‡∂¥‡∂≠‡∑ä‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂ã‡∂≠‡∑î‡∂ª‡∑î ‡∑É‡∑Ñ ‡∂Ø‡∂ö‡∑î‡∂´‡∑î ‡∂ö‡∑ú‡∂ª‡∑í‡∂∫‡∑è ‡∂ª‡∑è‡∂¢‡∑ä\\u200d‡∂∫‡∂∫‡∂±‡∑ä ‡∂Ö‡∂≠‡∂ª ‡∑É‡∑è‡∂∏ ‡∂ú‡∑í‡∑Ä‡∑í‡∑É‡∑î‡∂∏‡∂ö‡∂ß ‡∂ë‡∑Ö‡∂π‡∑ì‡∂∏ ‡∂Ø ‡∂∏‡∑ô‡∂∏ ‡∑É‡∑è‡∂ö‡∂†‡∑ä‡∂°‡∑è‡∑Ä‡∂Ω ‡∂Ö‡∂ª‡∂∏‡∑î‡∂´‡∂∫‡∑í',\n",
              " '‡∂ë‡∑É‡∑ö‡∂∏ 1 ‡∂¥‡∂ª‡∑è‡∂ö‡∑ä\\u200d‡∂ª‡∂∏‡∂∂‡∑è‡∑Ñ‡∑î ‡∂ª‡∂¢‡∑î ‡∂¥‡∂ª‡∑è‡∂ö‡∑ä\\u200d‡∂ª‡∂∏ ‡∑É‡∂∏‡∑î‡∂Ø‡∑ä\\u200d‡∂ª‡∂∫‡∑ö ‡∂¢‡∂Ω‡∂∫ ‡∂Ö‡∂©‡∑î ‡∂±‡∑ú‡∑Ä‡∑ì ‡∑É‡∑ä‡∂Æ‡∑è‡∑Ä‡∂ª‡∑Ä ‡∂≠‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏ ‡∑É‡∂Ø‡∑Ñ‡∑è 1‡∑Ä‡∂± ‡∂Ö‡∂ú‡∑ä‡∂∂‡∑ù ‡∂ª‡∂¢‡∑î ‡∂¥‡∑ô‡∂ª ‡∂â‡∂Ø‡∑í‡∂ö‡∑Ö ‡∂∏‡∑í‡∂±‡∑í‡∂¥‡∑ö ‡∂á‡∂Ω ‡∑Ä‡∑É‡∑ä‡∂ú‡∂∏‡∑î ‡∂î‡∂∫‡∑ö ‡∑É‡∑í‡∂ß ‡∂Ö‡∂π‡∂±‡∑ä ‡∂ú‡∂ü‡∂ß ‡∑É‡∂∏‡∑ä‡∂∂‡∂±‡∑ä‡∂∞ ‡∂ö‡∂ª‡∂± ‡∂Ω‡∂Ø‡∑ì ‡∂ä‡∂ß ‡∂Ö‡∂∏‡∂≠‡∂ª‡∑Ä ‡∂Ö‡∂π‡∂±‡∑ä ‡∂ú‡∂ü‡∑ö ‡∑Å‡∑è‡∂õ‡∑è ‡∂ú‡∂Ç‡∂ú‡∑è‡∑Ä‡∂ö‡∑ä ‡∑Ä‡∑ñ ‡∂ö‡∑Ö‡∑î ‡∂ú‡∂ü ‡∑Ñ‡∂ª‡∑É‡∑ä ‡∂ö‡∂ª ‡∂Ö‡∂∏‡∑î‡∂´‡∂ö‡∑ä ‡∂∂‡∑ê‡∂≥ ‡∂∫‡∑ù‡∂∞‡∂∫‡∑ù ‡∂∂‡∑ê‡∂≥‡∑ì ‡∂á‡∂Ω‡∂ö‡∑ä ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∂¢‡∂Ω‡∂∫ ‡∂Ö‡∂π‡∂±‡∑ä ‡∂ú‡∂ü‡∂ß ‡∂∫‡∑è ‡∂ö‡∂ª‡∂± ‡∂Ω‡∂Ø‡∑ì ‡∂∏‡∑ö ‡∑É‡∑í‡∂∫‡∂Ω‡∑î ‡∂á‡∂Ω ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú ‡∂¥‡∑ê‡∂ª‡∂´‡∑í‡∂∫‡∑ö ‡∂≠‡∑í‡∂∂‡∑î ‡∂Ö‡∂∫‡∑î‡∂ª‡∑í‡∂±‡∑ä ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∑É‡∂Ç‡∂ö‡∂ª‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂á‡∂≠‡∑í ‡∂±‡∑í‡∑É‡∑è ‡∂¥‡∑ê‡∂ª‡∂´‡∑í ‡∑Ñ‡∑ô‡∂Ω‡∂∫‡∑è‡∂ú‡∑ö ‡∑Ä‡∑í‡∑Å‡∑ä‡∂∏‡∑í‡∂≠ ‡∂≠‡∑è‡∂ö‡∑ä‡∑Ç‡∂±‡∂∫ ‡∑Ñ‡∑ú‡∂≥‡∑í‡∂±‡∑ä ‡∂Ö‡∑Ä‡∂∞‡∑è‡∂ª‡∂´‡∂∫ ‡∂ö‡∂ª‡∂ú‡∂≠ ‡∑Ñ‡∑ê‡∂ö ‡∂Ö‡∂Ø ‡∂Ø‡∂ö‡∑ä‡∂±‡∂ß ‡∂á‡∂≠‡∑í ‡∂¥‡∂ª‡∑è‡∂ö‡∑ä\\u200d‡∂ª‡∂∏ ‡∑É‡∂∏‡∑î‡∂Ø‡∑ä\\u200d‡∂ª‡∂∫‡∑ö ‡∑Ä‡∑ê‡∑Ä‡∑ä ‡∂∂‡∑ê‡∂∏‡∑ä‡∂∏‡∑ö ‡∂Ø‡∑í‡∂ú ‡∑É‡∑ê‡∂≠‡∂¥‡∑î‡∂∏‡∑ä 8 1 2 ‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´ ‡∑Ä‡∑ö ‡∑Ä‡∑ê‡∑Ä‡∑ä ‡∂∂‡∑ê‡∂∏‡∑ä‡∂∏‡∑ö ‡∂ã‡∑É ‡∂Ö‡∂©‡∑ä 40‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´‡∂∫ ‡∑Ä‡∂ª‡∑ä‡∂ú ‡∑É‡∑ê‡∂≠‡∂¥‡∑î‡∂∏‡∑ä 28‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´ ‡∑Ä‡∑ô‡∂Ω‡∑ä ‡∂∫‡∑è‡∂∫‡∂ö‡∂ß ‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∂¢‡∂Ω‡∂∫ ‡∑É‡∑ê‡∂¥‡∂∫‡∑í‡∂∏ ‡∂ö‡∂Ω ‡∑Ñ‡∑ê‡∂ö ‡∂∏‡∑ô‡∂∏ ‡∂¢‡∂Ω‡∑è‡∑Å‡∂∫ ‡∂Ø‡∑ô‡∑Ä‡∑ê‡∂±‡∑í ‡∑Ä‡∂±‡∑ä‡∂±‡∑ö ‡∂ú‡∂Ω‡∑ä‡∂î‡∂∫ ‡∂¢‡∂Ω‡∑è‡∑Å‡∂∫‡∂ß ‡∂¥‡∂∏‡∂´‡∑í ‡∑Ä‡∑ê‡∑Ä‡∑ä ‡∂∂‡∑ê‡∂∏‡∑ä‡∂∏‡∑ö ‡∂≠‡∑ê‡∂±‡∑í‡∂±‡∑ä ‡∂≠‡∑ê‡∂± ‡∂Ω‡∑ö‡∂õ‡∂± ‡∂ö‡∑ô‡∂ß‡∑î ‡∂ú‡∂Ω‡∑ä ‡∂ö‡∂±‡∑î‡∑Ä‡∑ô‡∂∫‡∑í ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω ‡∑Ñ‡∑ù ‡∑É‡∂Ç‡∑É‡∑ä‡∂ö‡∑ò‡∂≠ ‡∂∑‡∑è‡∑Ç‡∑è‡∑Ä‡∑ô‡∂±‡∑ä ‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∑í‡∂±‡∑ä ‡∂∏‡∑ô‡∂∏ ‡∂ú‡∂Ω‡∑ä‡∂ö‡∂´‡∑î ‡∂Ø‡∑ô‡∂¥‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö ‡∂Ö‡∂ö‡∑ä‡∑Ç‡∂ª ‡∂ö‡∑ú‡∂ß‡∑è ‡∂á‡∂≠ ‡∂ª‡∑í‡∂∫‡∂±‡∑ä ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∑É‡∂≥‡∑Ñ‡∂±‡∑ä ‡∂ö‡∂ª ‡∂á‡∂≠‡∑í ‡∂∏‡∑ô‡∂∏ ‡∂ú‡∂Ω‡∑ä‡∂ö‡∂´‡∑î‡∑Ä‡∂Ω ‡∑Ä‡∑ô‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\u200d‡∂ª‡∑É‡∑í‡∂Ø‡∑ä‡∂∞ ‡∂≠‡∑ê‡∂±‡∑ä‡∑Ä‡∂Ω ‡∂á‡∂≠‡∑í ‡∑Ä‡∑ê‡∑Ä‡∑ä ‡∂ö‡∂±‡∑ä‡∂©‡∑ä‡∑Ä‡∂Ω ‡∂Ø‡∑í‡∂ú ‡∑Ñ‡∑è ‡∑É‡∑É‡∂Ø‡∑è ‡∂≠‡∑í‡∂∂‡∑ö ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂Ω‡∑ô‡∑É 4 5 ‡∂ú‡∂Ω‡∑ä ‡∂ö‡∂´‡∑î ‡∂Ö‡∂≠‡∂ª ‡∂Ø‡∑í‡∂ú ‡∂ª‡∑í‡∂∫‡∂±‡∑ä 3200‡∂ö‡∑í ‡∂í ‡∂¥‡∂Ø‡∑Ä‡∑í‡∂∫ ‡∑Ä‡∑ê‡∑Ä‡∑ö ‡∑Ä‡∑ê‡∑Ä‡∑ä ‡∂ö‡∂±‡∑ä‡∂©‡∑í‡∂∫‡∑ö ‡∂Ø‡∑í‡∂ú‡∂∫‡∑í 5 6 ‡∂ú‡∂Ω‡∑ä ‡∂ö‡∂´‡∑î ‡∂Ö‡∂≠‡∂ª ‡∂Ø‡∑í‡∂ú ‡∂ª‡∑í‡∂∫‡∂±‡∑ä 1700‡∂ö‡∑í ‡∂í ‡∂ö‡∑Ö‡∑è ‡∑Ä‡∑ê‡∑Ä‡∑ö ‡∑Ä‡∑ë ‡∂ö‡∂±‡∑ä‡∂©‡∑í‡∂∫‡∑ö ‡∂Ø‡∑í‡∂ú‡∂∫‡∑í ‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∂¥‡∂ª‡∑è‡∂ö‡∑ä\\u200d‡∂ª‡∂∏ ‡∑É‡∂∏‡∑î‡∂Ø‡∑ä\\u200d‡∂ª‡∂∫‡∑ö ‡∑Ä‡∑í‡∑Å‡∑è‡∂Ω ‡∂∂‡∑Ä ‡∑É‡∑í‡∂≠‡∑è‡∂ú‡∂≠ ‡∑Ñ‡∑ê‡∂ö‡∑í‡∂∫',\n",
              " '‡∂î‡∑Ä‡∑î‡∑Ñ‡∑î ‡∑É‡∑í‡∑É‡∑î‡∂±‡∑ä ‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑ä ‡∂¥‡∑ä\\u200d‡∂ª‡∑è‡∂´‡∂á‡∂¥‡∂ö‡∂ª‡∑î‡∑Ä‡∂±‡∑ä ‡∑É‡∑ö ‡∂ª‡∂≥‡∑Ä‡∑è ‡∂≠‡∑ê‡∂∂‡∑î‡∑Ä‡∑ö ‡∑Ä‡∑í‡∂Ø‡∑ä\\u200d‡∂∫‡∑è ‡∂¥‡∑ì‡∂®‡∂∫‡∑ö ‡∂â‡∑Ñ‡∑Ö ‡∂∏‡∑è‡∂Ω‡∂∫‡∑ö ‡∂ö‡∑è‡∂∏‡∂ª‡∂∫‡∂ö‡∑ä ‡∂≠‡∑î‡∑Ö‡∂∫',\n",
              " '‡∂ö‡∑Ä‡∑í ‡∑Ñ‡∂Ø‡∂Ω‡∑è ‡∂Ω‡∑É‡∑ä‡∑É‡∂±‡∂ß ‡∂ö‡∑í‡∂∫‡∂ö‡∑í‡∂∫ ‡∂î‡∑Ä‡∑è ‡∂Ö‡∑Ä‡∑î‡∑É‡∑ä‡∑É‡∂±‡∑ä‡∂± ‡∂∫‡∂±‡∑ä‡∂± ‡∂ë‡∂¥‡∑è ‡∂Ö‡∂¥‡∑è‡∂∫‡∑ö ‡∂∫‡∑è‡∑Ä‡∑í ‡∂∏‡∑ö‡∂ö ‡∑Ñ‡∑ú‡∂Ø‡∂∫‡∑í',\n",
              " '‡∂Ö‡∂±‡∑ô‡∂ö ‡∑Ä‡∂±‡∑ä‡∂±‡∑ö ‡∂∂‡∑ê‡∂Ç‡∂ö‡∑î ‡∑É‡∑í‡∂∫‡∂Ω‡∑ä‡∂Ω‡∂∏ ‡∂∫‡∑î‡∂ª‡∑ù ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∂∫‡∂ß ‡∂¥‡∂ª‡∑í‡∂ú‡∂´‡∂ö ‡∂ú‡∂≠‡∂ö‡∑ú‡∂ß ‡∂á‡∂≠‡∑í ‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä ‡∂í‡∑Ä‡∑è ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∑É‡∂ö‡∑É‡∂±‡∑ä‡∂±‡∂ß ‡∂ö‡∂Ω‡∂ö‡∑ä ‡∂ú‡∂≠ ‡∑Ä‡∑ì‡∂∏‡∂∫‡∑í',\n",
              " '‡∂ë‡∂∫ ‡∂â‡∂ß‡∑ä‡∂ö‡∑è ‡∂ú‡∑í‡∑Ä‡∑í‡∑É‡∑î‡∂∏‡∑ö ‡∂∏‡∑ñ‡∂Ω‡∑í‡∂ö ‡∂Ö‡∂ª‡∂∏‡∑î‡∂´‡∑î‡∑Ä‡∂Ω‡∂ß ‡∂¥‡∑è‡∂ª ‡∂ö‡∑ê‡∂¥‡∑ì‡∂∏‡∂ö‡∑í',\n",
              " '‡∂Ö‡∂∑‡∑ä\\u200d‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∑Ñ‡∑è ‡∂¢‡∑è‡∂≠‡∑ä\\u200d‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂¥‡∑ì‡∂©‡∂±‡∂∫ ‡∂ë‡∑Ñ‡∑í‡∂Ω‡∑è ‡∂¥\\u200d‡∑ä\\u200d‡∂ª‡∂∏‡∑î‡∂õ‡∂∫',\n",
              " '‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂Ö‡∂ü‡∂Ω‡∑ä 7 ‡∂ö ‡∂¥‡∑ä\\u200d‡∂ª‡∂∏‡∑è‡∂´‡∂∫‡∑ô‡∂±‡∑ä ‡∂∫‡∑î‡∂≠‡∑î ‡∂≠‡∑í‡∂ª‡∂∫ 2048 1536 ‡∂¥‡∑ä\\u200d‡∂ª‡∂∏‡∑è‡∂´‡∂∫‡∑ö ‡∑Ä‡∑í‡∂∑‡∑ö‡∂Ø‡∂±‡∂∫‡∂ö‡∑í‡∂±‡∑ä ‡∂∫‡∑î‡∂ö‡∑ä‡∂≠ ‡∑Ä‡∂± ‡∂Ö‡∂≠‡∂ª ‡∂°‡∑è‡∂∫‡∑è‡∂ª‡∑ñ‡∂¥‡∂∫‡∂ö ‡∂∏‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ñ ‡∂¥‡∂ª‡∑í‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑É‡∑î‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∂≠‡∑è‡∑Ä‡∂ö‡∑ä ‡∂î‡∂∂‡∂ß ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ô‡∂±‡∑î ‡∂á‡∂≠',\n",
              " '‡∑Ñ‡∂ª‡∑í ‡∑Ñ‡∂ª‡∑í ‡∂î‡∂∫ ‡∂¥‡∑í‡∑É‡∑ä‡∑É‡∑î ‡∂∫‡∂ö‡∑ä‡∂ö‡∑î ‡∂ú‡∑ê‡∂± ‡∂∏‡∂Ø‡∑ê‡∂∫‡∑í ‡∂Ø‡∑ê‡∂Ç',\n",
              " '‡∂í ‡∂±‡∑í‡∑É‡∑è‡∂∏ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂∏‡∂∏ ‡∑Ñ‡∑í‡∂≠‡∑î‡∑Ä‡∑ö ‡∂±‡∑í‡∂ª‡∑ñ‡∂¥‡∑í‡∂ö‡∑è‡∑Ä‡∂ö‡∂ß ‡∑Ä‡∂©‡∑è ‡∂±‡∑í‡∑Ö‡∑í‡∂∫‡∂ö‡∑ä ‡∑Ä‡∑í‡∂Ø‡∑í‡∑Ñ‡∂ß ‡∂∏‡∂ú‡∑ö ‡∂ª‡∑É‡∑í‡∂ö‡∂∫‡∂±‡∑ä‡∂ß ‡∑Ö‡∂Ç ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è ‡∂±‡∂∏‡∑ä ‡∑Ñ‡∑ú‡∂≥‡∂∫‡∑í ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è',\n",
              " '‡∂ë‡∂Ø‡∑í‡∂± ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í‡∑Ä‡∂ª‡∂´‡∂∫‡∂ö‡∑ä ‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂Ö‡∂ú‡∂∏‡∑ê‡∂≠‡∑í‡∑Ä‡∂ª‡∂´‡∂∫‡∂ö‡∑ä ‡∂Ø ‡∂¥‡∑ê‡∑Ä‡∑ê‡∂≠‡∑ä ‡∑Ä‡∑ñ ‡∂∂‡∑Ä ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä ‡∂Ö‡∂¥‡∂ß ‡∂ö‡∑í‡∂∫‡∑è ‡∂Ø‡∑ô‡∂∫‡∑í',\n",
              " '‡∂Ω‡∑ú‡∂ö‡∑ñ‡∂ú‡∑ö ‡∑Ñ‡∑ú‡∂ª ‡∂ö‡∑ö‡∑É‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä‡∑Ä‡∂≠‡∑ä ‡∂©‡∂∫‡∑í‡∂±‡∂∏‡∂∫‡∑í‡∂ß‡∑ä ‡∂ë‡∂ö ‡∂∏‡∑è‡∂ß‡∑ä‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂∫‡∂±‡∑Ä‡∑è‡∂Ø ‡∂Ø‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ë',\n",
              " '‡∂¢‡∑ú‡∂∂‡∑ä ‡∂ë‡∂ö ‡∂±‡∑ê‡∂≠‡∑í ‡∑Ä‡∑î‡∂±‡∑î ‡∂ë‡∂ö ‡∂∏‡∂ß ‡∂±‡∂∏‡∑ä ‡∑É‡∑í‡∂Ø‡∑ä‡∂Ø ‡∑Ä‡∑î‡∂±‡∑î ‡∂Ω‡∑ú‡∂ö‡∑î‡∂∏ ‡∑Ñ‡∑ú‡∂≥‡∂ö‡∑ä',\n",
              " '‡∂∏‡∑ì‡∂ß ‡∂Ö‡∂∏‡∂≠‡∂ª‡∑Ä ‡∂¥‡∑É‡∑ä‡∑Ä‡∂± ‡∑Å‡∑ä\\u200d‡∂ª‡∑ö‡∂´‡∑í‡∂∫ ‡∑Å‡∑í‡∑Ç‡∑ä\\u200d‡∂∫‡∂≠‡∑ä‡∑Ä ‡∑Ä‡∑í‡∂∑‡∑è‡∂ú‡∂∫‡∑ö ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∂µ‡∂Ω ‡∂¥‡∂Ø‡∂±‡∂∏‡∑ä ‡∂ö‡∂ª‡∂ú‡∂±‡∑í‡∂∏‡∑í‡∂±‡∑ä ‡∂≠‡∂ª‡∂ú‡∂ö‡∑è‡∂ª‡∑ì‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂¥‡∑ä\\u200d‡∂ª‡∑Ä‡∂ª‡∑ä‡∂∞‡∂±‡∂∫ ‡∑Ä‡∂± ‡∂Ö‡∂∫‡∑î‡∂ª‡∑í‡∂±‡∑ä ‡∂¥‡∑ù‡∑É‡∑ä‡∂ß‡∂ª‡∑ä ‡∂∂‡∑ê‡∂±‡∂ª‡∑ä ‡∑Ä‡∑ê‡∂±‡∑í ‡∂¥‡∑ä\\u200d‡∂ª‡∂†‡∑è‡∂ª‡∂ö ‡∂ö‡∂ß‡∂∫‡∑î‡∂≠‡∑î ‡∂±‡∑ú‡∂ö‡∑Ö ‡∂∫‡∑î‡∂≠‡∑î ‡∂∂‡∑Ä‡∂Ø ‡∂∏‡∑ô‡∂∏ ‡∂†‡∂ö‡∑ä\\u200d‡∂ª ‡∂Ω‡∑ö‡∂õ‡∂∫‡∑ô‡∂±‡∑ä ‡∂Ö‡∑Ä‡∂∞‡∑è‡∂ª‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂≠‡∑í‡∂∂‡∑ö',\n",
              " '‡∂∂‡∂∫‡∑í‡∂ß‡∑ä ‡∂ë‡∂ö‡∂ö‡∂ß‡∂±‡∂∏‡∑ä ‡∂ë‡∂Ω‡∂∏ ‡∂ë‡∂Ω',\n",
              " '‡∂¢‡∑è‡∂≠‡∑í‡∑Ä‡∑è‡∂Ø‡∑ì ‡∂±‡∑ú‡∑Ä‡∑ô‡∂± ‡∑É‡∑î‡∂Ω‡∑î ‡∂¢‡∑è‡∂≠‡∑ì‡∂±‡∑ä 75 80 ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∑Å‡∂≠ ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂ë‡∂ö ‡∂¥‡∑ê‡∂≠‡∑ä‡∂≠‡∂ö‡∂ß ‡∂†‡∂±‡∑ä‡∂Ø‡∑ô ‡∂Ø‡∑ô‡∂±‡∑Ä‡∑è',\n",
              " '‡∂±‡∑ê‡∂≠‡∑í ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂Ø‡∑ô‡∂∫‡∂ö‡∑î‡∂≠‡∑ä ‡∂≠‡∑í‡∂∂‡∑î‡∂±‡∑ö ‡∂±‡∑ê‡∑Ñ‡∑ê‡∂∫‡∑í ‡∂ö‡∑í‡∂∫‡∂Ω‡∂≠‡∑ä ‡∂î‡∂∂‡∂ß ‡∂∫‡∑ù‡∂¢‡∂±‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑ä',\n",
              " '20‡∂∫‡∑í 20 ‡∂±‡∑è‡∂∫‡∂ö ‡∂¥‡∑í‡∂≠‡∑í‡∂ö‡∂ª‡∑î',\n",
              " '‡∂ï‡∂±‡∑ö ‡∑Ä‡∑ô‡∂Ω‡∑è‡∑Ä‡∂ß ‡∂ª‡∑ì‡∑É‡∑ä‡∂ß‡∑ù ‡∂ö‡∂ª‡∂ú‡∂±‡∑ä‡∂±',\n",
              " '‡∂∏‡∂±‡∑ä‡∂Ø ‡∂≠‡∑î‡∑Ä‡∂ö‡∑ä‡∂ö‡∑î‡∑Ä‡∑ö ‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂¥‡∑É ‡∂á‡∂≠‡∑í ‡∂Ö‡∂Ω‡∑ä‡∂Ω‡∂ú‡∂±‡∑ä‡∂±‡∑è ‡∂ö‡∑ú‡∂ß‡∑É ‡∂¥‡∑í‡∂ß‡∑î‡∂¥‡∑É‡∂ß ‡∑Ñ‡∑è ‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂∫‡∂ß ‡∂†‡∂Ω‡∂±‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ö‡∂Ø‡∑ì ‡∂¥‡∑ô‡∂ª ‡∑Ä‡∑è‡∂ª‡∂∫‡∑ö‡∂Ø‡∑ì ‡∑Ä‡∑ô‡∂©‡∑í ‡∂≠‡∑ê‡∂∂‡∑ì‡∂∏‡∂ß ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è‡∂ö‡∂Ω ‡∂¥‡∂≠‡∂ª‡∑ú‡∂∏ ‡∂â‡∑Ä‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏',\n",
              " '‡∂Ü‡∂Ø‡∑è‡∂∫‡∂∏‡∑ä ‡∂≠‡∂≠‡∑ä‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∑Ä‡∂ª‡∑ä‡∂∞‡∂±‡∂∫ ‡∑Ä‡∑ì‡∂∏ ‡∂±‡∑í‡∑É‡∑è ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂Ü‡∂∫‡∑ù‡∂¢‡∂± ‡∂ö‡∑ô‡∂ª‡∑ô‡∑Ñ‡∑í ‡∂î‡∂∂‡∂ú‡∑ö ‡∂Ö‡∑Ä‡∂∞‡∑è‡∂±‡∂∫ ‡∂∫‡∑ú‡∂∏‡∑î ‡∑Ä‡∑ô‡∂∫‡∑í',\n",
              " '‡∂í‡∂≠‡∑ä ‡∂ë‡∂∫ ‡∂±‡∑ì‡∂≠‡∑í‡∂∫‡∂ß ‡∑Ä‡∑í‡∂ª‡∑î‡∂Ø‡∑ä‡∂∞ ‡∂±‡∑ê‡∂≠‡∑í ‡∂±‡∑í‡∑É‡∑è ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß ‡∂ö‡∑í‡∑É‡∑í‡∑Ä‡∂ö‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂∂‡∑ê‡∂ª‡∑í ‡∑Ä‡∑î‡∂±‡∑è',\n",
              " '‡∂∏‡∑ö ‡∂Ø‡∑Ä‡∑É‡∑ä ‡∑Ä‡∂Ω ‡∑Ñ‡∑î‡∂ú‡∂ö‡∑ä ‡∂Ö‡∂∫ ‡∂ö‡∂≠‡∑è ‡∑Ä‡∑ô‡∂± ‡∂Ø‡∑ô‡∂∫‡∂ö‡∑ä‡∂±‡∑ô ‡∂Ö‡∂Ω‡∑î‡∂≠‡∑ä ‡∑É‡∂Ç‡∑É‡∑ä‡∂ö‡∂ª‡∂´‡∂∫ ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ô',\n",
              " '‡∂ë‡∂ö ‡∂ë‡∂ö‡∑è‡∂ß ‡∂ö‡∑è‡∂©‡∑ä ‡∂ú‡∑ê‡∑Ñ‡∑î‡∑Ä‡∂ß ‡∂ã‡∂π‡∑ö ‡∂ö‡∑è‡∂©‡∑ä ‡∂ö‡∑î‡∂©‡∑î',\n",
              " '‡∂∏‡∂∏ ‡∑Ñ‡∑ê‡∂∏‡∑ù‡∂ß‡∂∏ ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂± ‡∂ö‡∑ê‡∂∏‡∂≠‡∑í‡∂∫‡∑í ‡∑Å‡∑ä\\u200d‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è ‡∂∫‡∑î‡∂Ø ‡∑Ñ‡∂∏‡∑î‡∂Ø‡∑è‡∑Ä ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∑ä\\u200d‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∂±‡∑ä‡∑Ñ‡∑í ‡∂ö‡∂ß‡∂∫‡∑î‡∂≠‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∑É‡∑ê‡∑Ñ‡∑ô‡∂± ‡∂ö‡∑è‡∂Ω‡∂∫‡∂ö ‡∂â‡∂≥‡∂±‡∑ä ‡∂∫‡∑è‡∂¥‡∂±‡∂∫‡∑ö ‡∂ö‡∑ú‡∂ß‡∑î‡∑Ä ‡∂á‡∂≠‡∑î‡∑Ö‡∂≠ ‡∂∫‡∑î‡∂Ø ‡∑Ñ‡∂∏‡∑î‡∂Ø‡∑è‡∑Ä ‡∂â‡∂±‡∑ä‡∂± ‡∂¥‡∑ä\\u200d‡∂ª‡∂Æ‡∂∏ ‡∂Ö‡∑Ä‡∑É‡∑ä‡∂Æ‡∑è‡∑Ä ‡∂∏‡∑ô‡∂∫‡∂∏ ‡∂±‡∑ô‡∂∏‡∑ô‡∂∫‡∑í',\n",
              " '‡∂ë‡∂Ø‡∑è ‡∑Ñ‡∂∏‡∑î‡∂Ø‡∑è‡∂¥‡∂≠‡∑í ‡∑Ä‡∑ñ ‡∑É‡∂ª‡∂≠‡∑ä ‡∑ô‡∂±‡∑ä‡∑É‡∑ö‡∂ö‡∑è ‡∂¥‡∑Ä‡∑è ‡∂¥‡∑É‡∑î‡∑Ä ‡∂¥‡∑ê‡∑Ä‡∑É‡∑î‡∑Ä‡∑ö ‡∂∫‡∑î‡∂Ø‡∑ä‡∂∞‡∂∫‡∑ö ‡∂Ö‡∑Ä‡∑É‡∑è‡∂± ‡∑É‡∂∏‡∂∫‡∑ö ‡∂ö‡∑ú‡∂ß‡∑í ‡∑Ñ‡∂∏‡∑î‡∂Ø‡∑è‡∑Ä‡∂ß ‡∂∂‡∑è‡∂ª ‡∑Ä‡∑ì‡∂∏‡∂ß ‡∑Ä‡∑î‡∑Ä‡∂∏‡∂±‡∑è ‡∂∂‡∑Ä‡∂ß ‡∂ö‡∑í‡∑É‡∑í‡∂Ø‡∑î ‡∂≠‡∑ú‡∂ª‡∂≠‡∑î‡∂ª‡∂ö‡∑ä ‡∂≠‡∂∏‡∂±‡∑ä‡∂ß ‡∂±‡∑ú‡∂Ω‡∑ê‡∂∂‡∑î‡∂´‡∑î ‡∂∂‡∑Ä‡∂∫‡∑í.',\n",
              " '‡∂ë‡∑Ñ‡∑ô‡∂≠‡∑ä ‡∂Ö‡∂ª‡∑ä‡∂Æ ‡∂±‡∑í‡∂ª‡∑ñ‡∂¥‡∂´ ‡∂Ü‡∂•‡∑è ‡∂¥‡∂±‡∂≠‡∑ö 2 ‡∑Ä‡∂± ‡∑Ä‡∂ú‡∂±‡∑ä‡∂≠‡∑í‡∂∫ ‡∂Ö‡∂±‡∑î‡∑Ä ‡∑É‡∑Ö‡∂ö‡∑î‡∂´‡∂ö‡∑í‡∂±‡∑ä ‡∂Ö‡∂≠‡∑ä‡∑É‡∂±‡∑ä ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∂‡∑Ä ‡∂Ø‡∑ê‡∂ö‡∑ä\\u200d\\u200d‡∑Ä‡∑ô‡∂∫‡∑í',\n",
              " '‡∂¥‡∑ú‡∂±‡∑ä‡∑É‡∑ö‡∂ö‡∑è ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ö ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∂¥‡∂≠‡∑ä‡∂≠‡∑í‡∂∫‡∂ö‡∑ä ‡∂á‡∂≠‡∑í ‡∂∏‡∑í‡∂±‡∑í‡∑Ñ‡∑ô‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í',\n",
              " '‡∂í‡∂ö ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂á‡∂≠‡∑í ‡∂î‡∂∫ ‡∂ö‡∑í‡∂∫‡∂± ‡∑Ä‡∑ô‡∂±‡∑É',\n",
              " '‡∂ö‡∑ú‡∑Ö‡∂π ‡∂Ö‡∂Ø ‡∂ö‡∑ê‡∂Ø‡∑Ä‡∑è ‡∂≠‡∑í‡∂∂‡∑ñ ‡∂∏‡∑è‡∂∞‡∑ä\\u200d‡∂∫ ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä‡∑Ä‡∑ô‡∂∏‡∑í‡∂±‡∑ä ‡∂ë‡∑Ñ‡∑í ‡∂ö‡∑ê‡∂Ø‡∑Ä‡∑î‡∂∏‡∑ä‡∂ö‡∂ª‡∑î ‡∂Ω‡∑Ñ‡∑í‡∂ª‡∑î ‡∑Ä‡∑ì‡∂ª‡∑É‡∑ö‡∂ö‡∂ª ‡∂∏‡∑ö ‡∂∂‡∑Ä ‡∑É‡∂Ø‡∑Ñ‡∂±‡∑ä ‡∂ö‡∑Ö‡∑è',\n",
              " '‡∂ö‡∑è‡∑Å‡∑ä\\u200d‡∂∫‡∂¥ ‡∂ª‡∂¢‡∂≠‡∑î‡∂∏‡∑è ‡∂¥‡∑É‡∑î ‡∂∂‡∑Ñ‡∑í‡∂±‡∑Ä‡∑è ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è ‡∑Ñ‡∑í‡∂≠‡∂¥‡∑î ‡∑É‡∑ö‡∂±‡∑è‡∑Ä ‡∑É‡∑ì ‡∑É‡∑ì ‡∂ö‡∂© ‡∂Ø‡∑í‡∑Ä‡∑ä‡∑Ä‡∂Ω‡∑î',\n",
              " '‡∂í‡∂ö ‡∂±‡∑í‡∑É‡∑è ‡∂∂‡∑ö‡∂ª‡∑ô‡∂± ‡∂ë‡∂ö ‡∂ú‡∑ú‡∂©‡∂ö‡∑ä ‡∂Ö‡∂∏‡∑è‡∂ª‡∑î‡∂∫‡∑í',\n",
              " '‡∂Ö‡∂¥‡∑í ‡∂ã‡∂≠‡∑î‡∂ª‡∂ß ‡∂ú‡∑í‡∑Ñ‡∑í‡∂±‡∑ä ‡∂í‡∂ö ‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä‡∑è',\n",
              " '‡∂∂‡∑É‡∑ä ‡∂ª‡∂Æ‡∂∫ ‡∂¥‡∑ô‡∂ª‡∂Ω‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∑Ä‡∑Ñ‡∂Ω‡∂ß ‡∂Ø ‡∂Ø‡∑ê‡∂©‡∑í ‡∂Ö‡∂Ω‡∑è‡∂∑ ‡∑Ñ‡∑è‡∂±‡∑í ‡∑É‡∑í‡∂Ø‡∑î‡∑Ä ‡∂á‡∂≠‡∑í ‡∂Ö‡∂≠‡∂ª ‡∂∂‡∂´‡∑ä‡∂©‡∑è‡∂ª‡∑Ä‡∑ô‡∂Ω ‡∂¥‡∑ú‡∂Ω‡∑í‡∑É‡∑í‡∂∫ ‡∑Ä‡∑ê‡∂©‡∑í‡∂Ø‡∑î‡∂ª ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´ ‡∂¥‡∑Ä‡∂≠‡∑ä‡∑Ä‡∂±‡∑Ä‡∑è',\n",
              " '‡∂¢‡∑ú‡∂±‡∑ä ‡∑Ñ‡∑ä‡∂∫‡∑ú‡∂±‡∑ä ‡∂¢‡∑ù ‡∂¢‡∑ñ‡∂±‡∑ä ‡∂Ω‡∑ô‡∑É',\n",
              " '‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂¥‡∑ì‡∂©‡∂±‡∂∫‡∑ö ‡∂∂‡∂Ω‡∂¥‡∑ë‡∂∏ ‡∂±‡∑í‡∑É‡∑è ‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂±‡∑è‡∂Ω ‡∑É‡∑í‡∑Ñ‡∑í‡∂±‡∑ä ‡∑Ä‡∂± ‡∂∂‡∑Ä‡∂≠‡∑ä ‡∂í ‡∂±‡∑í‡∑É‡∑è ‡∂á‡∑É‡∑ä ‡∂Ö‡∂±‡∑ä‡∂∞‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑Ä‡∑è ‡∂∂‡∑ú‡∑Ñ‡∑ù ‡∂Ø‡∑î‡∂ª‡∂ß ‡∂â‡∂© ‡∂¥‡∑ä\\u200d‡∂ª‡∑É‡∑ä‡∂Æ‡∑è‡∑Ä ‡∂á‡∂≠‡∑í ‡∂∂‡∑Ä‡∂≠‡∑ä ‡∑É‡∑ú‡∂∫‡∑è‡∂ú‡∑ô‡∂± ‡∂á‡∂≠',\n",
              " '‡∂∏‡∑ö ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫‡∂ß ‡∂∏‡∑ö ‡∂±‡∑Ä ‡∂Ω‡∑í‡∂∂‡∂ª‡∂Ω‡∑ä‡∑Ä‡∑è‡∂Ø‡∑ì ‡∑Ä‡∑ä\\u200d‡∂∫‡∑è‡∂¥‡∑ò‡∂≠‡∑í‡∂∫‡∂ß ‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂∫‡∂ß ‡∂∫‡∂±‡∑ä‡∂± ‡∂Ø‡∑î‡∂±‡∑ä‡∂±‡∑ú‡∂≠‡∑ä ‡∂∏‡∑ö ‡∂ª‡∂ß‡∑ö ‡∑É‡∑ä‡∑Ä‡∂Ø‡∑ö‡∑Å‡∑í‡∂ö ‡∂¢‡∂±‡∂≠‡∑è‡∑Ä ‡∂∏‡∑ö ‡∂ª‡∂ß ‡∂ú‡∑í‡∂Ω‡∂ú‡∂±‡∑ä‡∂± ‡∂Ü‡∂¥‡∑î ‡∑Ä‡∑í‡∂Ø‡∑ö‡∑Å‡∑í‡∂ö ‡∑É‡∂∏‡∑è‡∂ú‡∂∏‡∑ä ‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂∫‡∑ö ‡∂Ö‡∑É‡∂ª‡∂´ ‡∑Ä‡∑ö‡∑Ä‡∑í',\n",
              " '‡∂Ö‡∂±‡∑ä‡∂± ‡∂í ‡∂±‡∑í‡∑É‡∑è‡∂∏‡∂∫‡∑í ‡∂∏‡∂∏ ‡∂ã‡∂π‡∂ú‡∑ê‡∂± ‡∂á‡∂Ω‡∂ª‡∑ä‡∂ß‡∑ä ‡∂ë‡∂ö‡∑ö ‡∂â‡∂Ø‡∂ú‡∂± ‡∂Ö‡∂Ø ‡∂ã‡∂π ‡∂ë‡∂ö‡∑ä‡∂ö ‡∑É‡∑ô‡∂ß‡∑ä‡∑Ä‡∑î‡∂±‡∑ö',\n",
              " '‡∂î‡∑Ñ‡∑î ‡∂ö‡∑í‡∂∫‡∂¥‡∑î ‡∂Ø‡∑ö ‡∂∏‡∂ß ‡∂≠‡∑è‡∂∏ ‡∂Ø‡∑ù‡∂±‡∑ä‡∂ö‡∑è‡∂ª ‡∂Ø‡∑ô‡∂±‡∑Ä‡∑è',\n",
              " '‡∂∏‡∂±‡∑î‡∂∏‡∑ä\\u200d‡∂∫‡∂∫ ‡∂∏‡∂≠‡∂ö ‡∂≠‡∑í‡∂∫‡∑è‡∂ú‡∂±‡∑í‡∂±‡∑ä ‡∂ã‡∂π‡∂ß ‡∂©‡∑ú‡∂ö‡∑ä‡∂ß‡∂ª‡∑ä ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ô ‡∂Ü‡∂†‡∑è‡∂ª‡∑ä‡∂∫ ‡∂ö‡∑í‡∂∫‡∂± ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑í‡∂±‡∑ä',\n",
              " '‡∂ö‡∑ú‡∂Ω‡∑ä‡∂Ω‡∂ú‡∑ô ‡∂Ö‡∂∏‡∑ä‡∂∏‡∂ß ‡∂≠‡∂∏‡∂∫‡∑í ‡∂ä‡∂∫‡∑ö ‡∂ª‡∑è‡∂≠‡∑ä‡∂≠‡∑í‡∂ª‡∑í‡∂∫‡∑ô ‡∑É‡∑î‡∂Ø‡∑î ‡∂ª‡∑ô‡∂Ø‡∑ä‡∂Ø‡∂ö‡∑ä ‡∂¥‡∑ô‡∂ª‡∑Ä ‡∂ú‡∂≠‡∑ä‡∂≠ ‡∂∏‡∑í‡∂∫‡∂ú‡∑í‡∂∫ ‡∂¥‡∂ª‡∑è‡∂± ‡∂ö‡∑è‡∂ª‡∂∫‡∑ô‡∂ö‡∑ä ‡∑Ñ‡∑ì‡∂±‡∑ô‡∂±‡∑ä ‡∂¥‡∑ô‡∂±‡∑í‡∂Ω ‡∂ö‡∑í‡∂∫‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑ä‡∂±‡∑ô',\n",
              " '‡∂ë ‡∂ö‡∑ú‡∂Ω‡∑ä‡∂Ω‡∑è ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂Ø‡∑ê‡∂±‡∂ú‡∂≠‡∑ä‡∂≠‡∑ö ‡∂±‡∑ê‡∂Ø‡∑ä‡∂Ø ‡∂Ø‡∑î‡∂±‡∑ä‡∂± ‡∂Ω‡∂´‡∑î‡∑Ä.',\n",
              " '‡∑Ä‡∑ô‡∂Ω‡∑è‡∑Ä‡∂ö‡∑ä ‡∑Ñ‡∂Ø‡∑è‡∂ú‡∑ô‡∂± ‡∂∫‡∂∏‡∑î ‡∂Ø‡∑Ä‡∑É‡∂ö',\n",
              " '‡∂ú‡∂≠‡∑Ä‡∑î ‡∑É‡∂≠‡∑í‡∂∫‡∑ö ‡∂Ø‡∑ö‡∑Å‡∂¥‡∑è‡∂Ω‡∂± ‡∑Ä‡∑ö‡∂Ø‡∑í‡∂ö‡∑è‡∑Ä‡∑ö ‡∂¥‡∑Ö ‡∑Ä‡∑ñ ‡∂ª‡∑É‡∑Ä‡∂≠‡∑ä ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ª‡∑ê‡∂ú‡∂≠‡∑ä ‡∂ª‡∑É ‡∂ö‡∂≠‡∑è ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç‡∑è‡∂Ç‡∂ú‡∂∫ ‡∂≠‡∑î‡∑Ö‡∑í‡∂±‡∑ä ‡∂Ö‡∂Ø ‡∂î‡∂∂ ‡∑Ä‡∑ô‡∂≠ ‡∂ú‡∑ô‡∂± ‡∂ë‡∂±‡∑ä‡∂±‡∑ö ‡∑Ä‡∑í‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂±‡∑è‡∂∫‡∂ö ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä ‡∑Ä‡∑í‡∂ö\\u200d‡∑ä\\u200d‡∂ª‡∂∏‡∑É‡∑í‡∂Ç‡∑Ñ ‡∂Ö‡∂∏‡∑è‡∂≠‡∑ä\\u200d‡∂∫ ‡∂ö‡∑î‡∂∏‡∑è‡∂ª ‡∑Ä‡∑ô‡∂Ω‡∑ä‡∂ú‡∂∏ ‡∑Ñ‡∑è ‡∂¥‡∑è‡∂ª‡∑ä‡∂Ω‡∑í‡∂∏‡∑ö‡∂±‡∑ä‡∂≠‡∑î ‡∂∏‡∂±‡∑ä‡∂≠\\u200d‡∑ä\\u200d‡∂ª‡∑ì ‡∑Ñ‡∂ª‡∑ì‡∂±‡∑ä ‡∂¥\\u200d‡∑ä\\u200d‡∂ª‡∂±‡∑è‡∂±‡∑ä‡∂Ø‡∑î ‡∂Ø‡∑ö‡∑Å‡∂¥‡∑è‡∂Ω‡∂± ‡∑Ä‡∑ö‡∂Ø‡∑í‡∂ö‡∑è‡∑Ä‡∑ö ‡∂Ø‡∑ê‡∂ö‡∑ä‡∑Ä‡∑ñ ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä',\n",
              " '‡∂á‡∂≠‡∑ä‡∂≠‡∂ß‡∂∏ ‡∂¥‡∑í‡∂±‡∑ä ‡∑Ñ‡∂≠‡∑ö ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∂ö‡∂ª‡∂Ω‡∑è ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑ä‡∂±‡∑ô',\n",
              " '‡∂ë‡∂Ø‡∑è ‡∂∏‡∑ô‡∂Ø‡∑è ‡∂≠‡∑î‡∂ª ‡∂Ω‡∑ù ‡∂¥\\u200d‡∑ä\\u200d‡∂ª\\u200d‡∑ä\\u200d‡∂ª‡∂ö‡∂ß‡∑Ä‡∑ñ ‡∂±‡∑è‡∂ß‡∑ä\\u200d‡∂∫ ‡∂ª‡∑ê‡∑É‡∂ö‡∑ä ‡∂í‡∑Ä‡∑è ‡∂∏‡∑î‡∂Ω‡∑í‡∂±‡∑ä‡∂∏ ‡∂ö‡∂ª‡∂Ω‡∑í‡∂ú‡∂≠ ‡∑Ä‡∂±‡∑Ä‡∑í‡∂ß ‡∑Ä‡∑è‡∂ª‡∂´‡∂∫‡∂±‡∑ä‡∂ß ‡∂Ω‡∂ö‡∑ä‡∑Ä ‡∂á‡∂≠',\n",
              " '‡∂î‡∂±‡∑ä‡∂± ‡∂∏‡∂∏ ‡∂ã‡∂´‡∑î ‡∂ã‡∂´‡∑î‡∑Ä‡∑ô‡∂∏ ‡∂ú‡∂≠‡∑ä‡∂≠‡∑è',\n",
              " '‡∂ß‡∑ô‡∂Ω‡∑í ‡∂±‡∑è‡∂ß‡∑ä\\u200d‡∂∫‡∑Ä‡∂Ω‡∂ß ‡∑Ä‡∂ú‡∑ö‡∂∏ ‡∂Ø‡∑ö‡∑Å‡∂¥‡∑è‡∂Ω‡∂±‡∂∫‡∂ß‡∂≠‡∑ä ‡∂í ‡∂ö‡∑è‡∂Ω‡∑ô ‡∂â‡∂≥‡∂±‡∑ä‡∂∏ ‡∂Ü‡∂ª‡∑è‡∂∞‡∂±‡∑è ‡∂Ω‡∑ê‡∂∂‡∑î‡∂´‡∑è',\n",
              " '‡∂ö‡∑ú‡∑Ö‡∂π ‡∑Ä‡∂ª‡∑è‡∂∫‡∂ß ‡∂¥‡∑í‡∑Ä‡∑í‡∑É‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∂∏‡∑ô‡∂≠‡∑ô‡∂ö‡∑ä ‡∂Ö‡∂∫ ‡∂ö‡∂Ω ‡∂ú‡∑è‡∑É‡∑ä‡∂≠‡∑î‡∑Ä ‡∑É‡∑í‡∂∫‡∂∫‡∂ß 4000 ‡∂ö‡∑í‡∂±‡∑ä ‡∂â‡∑Ñ‡∑Ö ‡∂±‡∑ê‡∂Ç‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂ë‡∂ª‡∑ô‡∑Ñ‡∑í‡∑Ä ‡∑Ñ‡∑ô‡∂ß ‡∑É‡∑í‡∂ß ‡∑Ä‡∑ò‡∂≠‡∑ä‡∂≠‡∑ì‡∂∫ ‡∂ö‡∑ä\\u200d‡∂ª‡∑í‡∂∫‡∑è‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫‡∂ö ‡∂±‡∑í‡∂ª‡∂≠‡∑Ä‡∂± ‡∂∂‡∑Ä ‡∑Ä‡∂ª‡∑è‡∂∫ ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä‡∂ú‡∑ö ‡∑É‡∂Ç‡∂ú‡∂∏‡∑ä ‡∂ë‡∂ö‡∂∏‡∑î‡∂≠‡∑î‡∑Ä ‡∂¥‡∑ä\\u200d‡∂ª‡∂ö‡∑è‡∑Å ‡∂ö‡∑Ö‡∑è',\n",
              " '‡∂â‡∂±‡∑ä‡∂¥‡∑É‡∑î ‡∂≠‡∑Ä‡∂Ø‡∑î‡∂ª‡∂ß‡∂≠‡∑ä ‡∂ö‡∂ß‡∂∫‡∑î‡∂≠‡∑î ‡∂ö‡∂ª‡∂ú‡∑ô‡∂± ‡∂∫‡∑è‡∂∏ ‡∂î‡∂∂ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ì ‡∂¥‡∑è‡∂ª‡∑ä‡∑Å‡∑Ä‡∂±‡∑ä‡∂ú‡∑ö ‡∂ã‡∑Ä‡∂∏‡∂±‡∑è‡∑Ä ‡∂ö‡∑ê‡∂∏‡∑ê‡∂≠‡∑ä‡∂≠ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∂ü‡∂≠‡∑è‡∑Ä‡∂∫ ‡∂∏‡∂≠ ‡∂¥‡∂∏‡∂´‡∂ö‡∑ä ‡∑É‡∑í‡∂Ø‡∑î‡∑Ä‡∑ö',\n",
              " '‡∂ë‡∑Ñ‡∑ô‡∂≠‡∑ä ‡∂Ø‡∑ê‡∂±‡∑ä ‡∂â‡∂±‡∑ä ‡∂ë‡∂ö ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç‡∂∫‡∂ö‡∑ä ‡∑Ñ‡∑ê‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∑É‡∑í‡∂∫‡∂Ω‡∑ä‡∂Ω ‡∂±‡∑Ç‡∑ä‡∂ß‡∂¥\\u200d‡∑ä\\u200d‡∂ª‡∑è‡∂¥‡∑ä‡∂≠‡∂∫',\n",
              " '‡∂±‡∑ê‡∂±‡∑ù ‡∂≠‡∑è‡∂ö‡∑ä‡∑Ç‡∂±‡∂∫ ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂Ø ‡∂á‡∂≠‡∑í ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∂¥‡∑í‡∂ß‡∑î ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑ä',\n",
              " '‡∂∏‡∂∏ ‡∂ö‡∂≠‡∑è‡∑Ä‡∂ß ‡∂ö‡∑ô‡∑Ö‡∑í‡∂±‡∑ä‡∂∏ ‡∂ë‡∂±‡∑ä‡∂±‡∂∏‡∑ä',\n",
              " '‡∂∏‡∂∏ ‡∂∏‡∑ô‡∂†‡∑ä‡∂†‡∂ª ‡∂ö‡∂Ω‡∑ä ‡∑Ñ‡∑í‡∂≠‡∂±‡∑ä ‡∑Ñ‡∑í‡∂ß‡∑í‡∂∫‡∑ô ‡∑É‡∑í‡∂ª‡∑É ‡∂í ‡∑Ö‡∂∏‡∂∫‡∑í‡∂±‡∑ä‡∑Ä ‡∂ú‡∑í‡∑Ä‡∑í‡∑É‡∑î‡∂∏‡∑ä ‡∑Ä‡∂Ω ‡∑É‡∑í‡∂ª ‡∂ö‡∂ª‡∂ú‡∑ô‡∂± ‡∑Ñ‡∂∏‡∑ä‡∂∂ ‡∂ö‡∂ª‡∂±‡∑Ä ‡∂ö‡∑í‡∂∫‡∂Ω',\n",
              " '‡∂â‡∂≠‡∑í‡∂±‡∑ä ‡∂∏‡∂Ç ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂± ‡∑Ñ‡∑ê‡∂Ø‡∑î‡∑Ä‡∑ô ‡∂Ö‡∂¥‡∑í ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø‡∑î ‡∂ú‡∂´‡∂±‡∑ä ‡∂ú‡∑ô‡∑Ä‡∂Ω ‡∂Ω‡∑ú‡∂ö‡∑î ‡∑Ä‡∂∫‡∑É‡∂ö‡∂ß ‡∂ë‡∂Ø‡∑ä‡∂Ø‡∑í ‡∂Ö‡∂¥‡∑í‡∑Ä ‡∑Ñ‡∂≠‡∂ª‡∑ê‡∑É‡∑ä ‡∂ª‡∑è‡∂∏‡∑î‡∑Ä‡∂ö‡∂ß ‡∂ö‡∑ú‡∂ß‡∑î ‡∑Ä‡∑ô‡∂±‡∑Ä.‡∂ë‡∂ö‡∑ä‡∂ö‡∑ú ‡∂ö‡∑ú‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑Ä.‡∂ú‡∑ô‡∂Ø‡∂ª ‡∑Ä‡∂ß‡∂¥‡∑í‡∂ß‡∑è‡∑Ä‡∑ô‡∂±‡∑ä ‡∂ú‡∑ô‡∂Ø‡∂ª‡∑í‡∂±‡∑ä ‡∂ë‡∂¥‡∑í‡∂ß ‡∑É‡∂∏‡∑è‡∂¢‡∂∫‡∑ô‡∂±‡∑ä',\n",
              " '‡∂ë‡∂∏ ‡∂±‡∑í‡∑Ç‡∑ä‡∂¥‡∑è‡∂Ø‡∂± ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∑Ä‡∑í‡∂∞‡∑è‡∂ö‡∑è‡∂ª ‡∑É‡∑û‡∂õ‡∑ä\\u200d‡∂∫ ‡∂ú‡∑ê‡∂ß‡∑Ö‡∑î ‡∂∏‡∂≠‡∑î‡∑Ä‡∂± ‡∂Ö‡∂≠‡∂ª ‡∂∏‡∑î‡∂õ ‡∂¥‡∑í‡∑Ö‡∑í‡∂ö‡∑è ‡∂ú‡∂Ω‡∂±‡∑è‡∂Ω‡∂∫‡∑ö ‡∂¥‡∑í‡∑Ö‡∑í‡∂ö‡∑è ‡∂∏‡∑è‡∂±‡∑É‡∑í‡∂ö ‡∂ª‡∑ù‡∂ú ‡∑Ñ‡∑è ‡∂∂‡∂©‡∑Ä‡∑ê‡∂Ω‡∑ä ‡∂Ü‡∑Å‡∑ä\\u200d‡∂ª‡∑í‡∂≠ ‡∂ª‡∑ù‡∂ú ‡∂á‡∂≠‡∑í‡∑Ä‡∂± ‡∂∂‡∑Ä ‡∂Ö‡∂±‡∑è‡∑Ä‡∂ª‡∂´ ‡∑Ä‡∑ì ‡∂≠‡∑í‡∂∂‡∑ö',\n",
              " '‡∂∂‡∑ú‡∑Ñ‡∑ù ‡∑Ä‡∑í‡∂ß ‡∂Ö‡∂Ø‡∑è‡∂Ω ‡∑Ä‡∑í‡∑Ç‡∂∫ ‡∂∑‡∑è‡∂ª ‡∂á‡∂∏‡∂≠‡∑í ‡∑Ñ‡∑ù ‡∑Ä‡∑ô‡∂±‡∂≠‡∑ä ‡∂ª‡∑è‡∂¢‡∑ä\\u200d‡∂∫ ‡∂Ö‡∂∞‡∑í‡∂ö‡∑è‡∂ª‡∑í‡∂∫‡∂ö‡∑ä ‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑î‡∂∫‡∑í ‡∂ª‡∑ô‡∂ú‡∑î‡∂Ω‡∑è‡∑É‡∑í ‡∂¥‡∂±‡∑Ä‡∂±‡∑ä‡∂±‡∑ö',\n",
              " '‡∂∏‡∑î‡∂Ω ‡∂â‡∂≥‡∂Ω ‡∂ö‡∑í‡∂∫‡∑Ä‡∂±‡∑ä‡∂± ‡∂ú‡∂≠‡∑ä‡∂≠',\n",
              " '‡∂∏‡∑ô‡∂∏ ‡∂ú‡∑î‡∑Ä‡∂±‡∑ä ‡∂∫‡∑è‡∂±‡∂∫ ‡∂∏‡∂≠‡∑ä‡∂≠‡∂Ω ‡∂¢‡∑è‡∂≠‡∑ä\\u200d‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂ú‡∑î‡∑Ä‡∂±‡∑ä‡∂≠‡∑ú‡∂ß‡∑î‡∂¥‡∂Ω‡∂ß ‡∂ú‡∑ú‡∂© ‡∂∂‡∑ê‡∑É‡∑ä‡∑É‡∑Ä‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä ‡∂∏‡∑í‡∂Ω‡∑í‡∂∫‡∂± 15 ‡∂ö ‡∂Ü‡∂Ø‡∑è‡∂∫‡∂∏‡∂ö‡∑ä ‡∂Ω‡∑ê‡∂∂‡∑î‡∂´‡∑î ‡∂∂‡∑Ä‡∂∫‡∑í ‡∂ú‡∑î‡∑Ä‡∂±‡∑ä ‡∂≠‡∑ú‡∂ß‡∑î‡∂¥‡∑Ö ‡∂ö‡∑Ö‡∂∏‡∂´‡∑è‡∂ö‡∂ª‡∑î ‡∂ã‡∂¥‡∑î‡∂Ω‡∑ä ‡∂ö‡∂Ω‡∂Ç‡∑É‡∑ñ‡∂ª‡∑í‡∂∫ ‡∑Ñ‡∑í‡∂ª‡∑î ‡∂¥‡∑ä\\u200d‡∂ª‡∑Ä‡∑ò‡∂≠‡∑ä‡∂≠‡∑í ‡∂Ö‡∂Ç‡∑Å‡∂∫‡∂ß ‡∂ö‡∑í‡∂∫‡∑è ‡∑É‡∑í‡∂ß‡∑í‡∂∫‡∑ö',\n",
              " '‡∂ª‡∂ß‡∂∏ ‡∑Ñ‡∑í‡∂±‡∑ê‡∑É‡∑ä‡∑É‡∑ñ ‡∂¥‡∑ä\\u200d‡∂ª‡∑Ä‡∑ì‡∂´ ‡∂ª‡∂Ç‡∂ú‡∂± ‡∑Å‡∑í‡∂Ω‡∑ä‡∂¥‡∑ì ‡∑É‡∑î‡∂±‡∑í‡∂Ω‡∑ä ‡∑Ñ‡∑ô‡∂ß‡∑ä‡∂ß‡∑í ‡∂Ü‡∂ª‡∂†',\n",
              " '‡∂∏‡∑ö‡∂ö‡∑ö ‡∂Ø‡∑ê‡∂±‡∑ä ‡∂≠‡∑í‡∂ª ‡∂ª‡∂†‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ë‡∂±‡∑ö',\n",
              " '‡∑É‡∂Ç ‡∑Ä‡∂ª‡∑è ‡∂¥‡∑í‡∂ª‡∑í‡∑É‡∑î‡∂Ø‡∑î‡∑Ä ‡∑Å‡∑î‡∂Ø‡∑ä‡∂∞ ‡∂ö‡∂ª ‡∂ú‡∂≠ ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∂‡∑Ä‡∂≠‡∑ä ‡∂Ø‡∑ê‡∂ö‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫‡∑í',\n",
              " '‡∂∏‡∑ô‡∑Ñ‡∑í‡∂Ø‡∑ì ‡∑Å‡∑ä\\u200d‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è ‡∂â‡∂±‡∑í‡∂∏ ‡∂ú‡∑ú‡∂©‡∂±‡∑ê‡∂ú‡∑ì‡∂∏ ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∂ö‡∑ê‡∂¥‡∑ì ‡∂¥‡∑ô‡∂±‡∑ô‡∂± ‡∂Ø‡∑è‡∂∫‡∂ö‡∂≠‡∑ä‡∑Ä‡∂∫‡∂ö‡∑ä ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑î‡∂±‡∑ä ‡∂∑‡∑è‡∂±‡∑î‡∂ö ‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂¥‡∂±‡∑ä‡∂Ø‡∑î 48‡∂ö‡∂ß ‡∂∏‡∑î‡∑Ñ‡∑î‡∂´ ‡∂Ø\\u200d‡∑ô‡∂∏‡∑í‡∂±‡∑ä ‡∂Ö‡∂ú‡∂±‡∑è ‡∂Ω‡∂ö‡∑î‡∂´‡∑î 77‡∂ö‡∑ä ‡∂ª‡∑ê‡∑É‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∑É‡∂∏‡∂≠‡∑ä ‡∑Ä‡∑í‡∂∫',\n",
              " '‡∂Ø‡∑í ‡∂∏‡∑î ‡∂Ö‡∑É‡∑ä‡∂±‡∑ú‡∂ö‡∂ª ‡∂∏ ‡∂∏‡∑õ‡∂≠‡∑ä\\u200d‡∂ª‡∑í‡∂¥‡∑è‡∂Ω ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä ‡∂Ö‡∂ú‡∂∏‡∑ê‡∂≠‡∑í ‡∂Ω‡∑ô‡∑É ‡∂¥‡∂≠‡∑ä ‡∂ö‡∑Ö‡∑è',\n",
              " '‡∂Ø‡∑ê‡∂±‡∑ä ‡∂±‡∂∏‡∑ä ‡∂±‡∂∏‡∂∫‡∂ß ‡∑Ä‡∑í‡∂≠‡∂ª ‡∂±‡∑Ä‡∂≠‡∑í‡∂±‡∑Ä‡∑è',\n",
              " '‡∂î‡∂∫ ‡∂Ö‡∂≠‡∂ª‡∑ö ‡∂¥‡∑ô‡∂±‡∑ä‡∂Ø‡∑í‡∂ß ‡∂ß‡∑ä\\u200d‡∂ª‡∂∫‡∑í ‡∂ö‡∂ª‡∂¥‡∑î ‡∂ö‡∑ú‡∂Ω‡∑ä‡∂Ω‡∂≠‡∑ä ‡∂ã‡∂´‡∑î‡∑Ñ‡∑î‡∂∏',\n",
              " '‡∂∏‡∑ô‡∂≠‡∂± ‡∂∏‡∑ú‡∂ö‡∑ö‡∂ö‡∑ä ‡∑Ñ‡∂ª‡∑í ‡∂∏‡∂ú‡∑ö ‡∂Ö‡∂∫‡∑í‡∂∫‡∂ß ‡∂¥‡∑è‡∂ª‡∑ä‡∂ß‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ë‡∑Ñ‡∑ô‡∂∏ ‡∂Ø‡∑è‡∂Ω‡∑è ‡∂≠‡∑í‡∂∂‡∑ä‡∂∂‡∑ú‡∂≠‡∑ä ‡∂∂‡∂Ω‡∑è‡∂ú‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑ä ‡∑Ñ‡∂ª‡∑í‡∂∫',\n",
              " '1993 ‡∂¥‡∑î‡∂±‡∂ú‡∂ª‡∑í‡∂±‡∑ä ‡∂ö‡∂≥‡∑Ä‡∑î‡∂ª ‡∑Ä‡∑í‡∂∏‡∑î‡∂ö‡∑ä‡∂≠‡∑í ‡∂ö‡∑ú‡∂ß‡∑í ‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑ä ‡∂Ö‡∂≠‡∑ä‡∂¥‡∂≠‡∑ä ‡∂ö‡∂ª‡∂ú‡∂±‡∑ä‡∂±‡∑è ‡∂Ω‡∂Ø‡∑í',\n",
              " '‡∂Ö‡∂±‡∑ä‡∂≠‡∂ª‡∑ä ‡∑É‡∂∏‡∑è‡∂¢ ‡∂¥‡∑Ö‡∂∏‡∑î ‡∂ö‡∑ú‡∂ß‡∑É‡∑ö ‡∑Ä‡∑í‡∑É‡∑ä‡∑É‡∂∫‡∑í20 ‡∂≠‡∂ª‡∂ú‡∑è‡∑Ä‡∂Ω‡∑í‡∂∫‡∑ö‡∂Ø‡∑ì ‡∑Å‡∂≠‡∂ö ‡∂Ø‡∑ô‡∂ö‡∂ö‡∑ä ‡∂ª‡∑ê‡∑î‡∑É‡∑ä‡∂ö‡∑Ö ‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∑Å‡∑è‡∂±‡∂ö ‡∑É‡∂Ç‡∂†‡∑í‡∂≠‡∂∫‡∂ß ‡∂ö‡∑ê‡∂Ø‡∑Ä‡∑è ‡∑É‡∑í‡∂ß‡∑í‡∂∫‡∂Ø ‡∂â‡∂ö‡∑î‡∂≠‡∑ä ‡∂Ö‡∂ú‡∑Ñ‡∂ª‡∑î‡∑Ä‡∑è‡∂Ø‡∑è ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∑Ä‡∑ñ ‡∂Ö‡∂±‡∑ä‡∂≠‡∂ª‡∑ä ‡∂ö‡∂Ω‡∑è‡∂¥ ‡∑Ä‡∑í',\n",
              " '‡∂±‡∑í‡∂ö‡∂∏‡∂ß ‡∑Ñ‡∑í‡∂≠‡∂±‡∑ä‡∂± ‡∑Ä‡∑í‡∂∏‡∂Ω‡∑ä ‡∑Ä‡∑ì‡∂ª‡∑Ä‡∂Ç‡∑Å ‡∂ö‡∑ì‡∑Ä‡∑è‡∂ö‡∑ä ‡∂∏‡∑ô‡∂±‡∑ä ‡∂ª‡∑è‡∂≠‡∑ä\\u200d‡∂ª‡∑ì ‡∂±‡∑í‡∂Ø‡∂±‡∑ä‡∂±‡∂ß ‡∂∫‡∂± ‡∑Ä‡∑í‡∂ß ‡∂î‡∂∂‡∑ö ‡∂Ø‡∂ª‡∑î‡∑Ä‡∂±‡∑ä ‡∂∏‡∑è‡∂≠‡∑î ‡∂¥‡∑è‡∂Ø‡∂±‡∑ä ‡∂±‡∂∏‡∑è‡∂∏‡∑ì ‡∂∫‡∂±‡∑ä‡∂± ‡∑É‡∑ú‡∂¥‡∑ä\\u200d‡∂ª‡∑è‡∂´‡∑ù ‡∂ö‡∑ä\\u200d‡∂ª‡∂∏‡∂∫‡∂ß ‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä‡∑ú‡∂≠‡∑ä ‡∂ö‡∑ú‡∑Ñ‡∑ú‡∂∏‡∂ß ‡∑Ñ‡∑í‡∂ß‡∑ì‡∂Ø',\n",
              " '‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂¥‡∂≠‡∑ä ‡∂ö‡∂ª‡∂±‡∑î ‡∂Ω‡∑ê‡∂∂‡∑ñ ‡∂∏‡∑ô‡∂∏ ‡∂Ω‡∑í‡∂¥‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂Ö‡∂≠‡∑ö ‡∂á‡∂ú‡∑í‡∂Ω‡∑í ‡∂Ö‡∂≠‡∂ª ‡∂á‡∂≠‡∑í ‡∂¥‡∂ª‡∂≠‡∂ª‡∂∫ ‡∑Ä‡∑í‡∂∏‡∑É‡∑è ‡∂∂‡∂Ω‡∑è ‡∂¥‡∑î‡∂Ø‡∑ä‡∂ú‡∂Ω‡∂∫‡∑ô‡∂ö‡∑î‡∂ú‡∑ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∑É‡∑ä‡∑Ä‡∂∑‡∑è‡∑Ä‡∂∫ ‡∑Ä‡∑í‡∑Å‡∑ä‡∂Ω‡∑ö‡∑Ç‡∂´‡∂∫ ‡∂ö‡∂ª‡∂± ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂î‡∂∂ ‡∂Ø‡∑ê‡∂±‡∂ú‡∂±‡∑ä‡∂±‡∂ß ‡∂á‡∂≠‡∑í ‡∂ö‡∑í‡∂∫‡∑è ‡∂Ö‡∂¥ ‡∑É‡∑í‡∂≠‡∂±‡∑Ä‡∑è',\n",
              " '‡∂ö‡∑ê‡∂ª‡∂∏‡∑ä ‡∂∂‡∑ù‡∂û‡∑ä ‡∂Ü‡∂±‡∂∫‡∂±‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ö‡∂Ø‡∑ì ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä ‡∂∏‡∑í‡∂Ω‡∑í‡∂∫‡∂± ‡∂ú‡∂´‡∂±‡∂ö ‡∑Ä‡∂Ç‡∂†‡∑è‡∑Ä‡∂ö‡∑ä ‡∑É‡∑í‡∂Ø‡∑î‡∑Ä ‡∂á‡∂≠‡∑í ‡∂∂‡∑Ä‡∂ß‡∂∫‡∑í ‡∂î‡∑Ñ‡∑î‡∂ß ‡∂†‡∑ù‡∂Ø‡∂±‡∑è ‡∂ë‡∂Ω‡∑ä‡∂Ω ‡∑Ä‡∑ì ‡∂á‡∂≠‡∑ä‡∂≠‡∑ö',\n",
              " '‡∂∏‡∑ö ‡∂Æ‡∑î‡∂¥‡∂∫‡∑ö ‡∑Ä‡∑í‡∑Ç‡∑ä‡∂ö‡∂∏‡∑ä‡∂∑‡∂∫ ‡∂Ö‡∂©‡∑í 370‡∂ö‡∑í',\n",
              " '‡∂í‡∂ö‡∂∫‡∑í ‡∑Ä‡∑ô‡∂±‡∑É ‡∂Ö‡∂¥‡∑í ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂ú‡∑ô ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂Ø‡∑ô‡∂ö‡∑ö.',\n",
              " '‡∂ö‡∑î‡∂Ω‡∂¥‡∑ì‡∂©‡∂±‡∂∫‡∂ß ‡∂ë‡∂ª‡∑ô‡∑Ñ‡∑í‡∑Ä ‡∂â‡∂±‡∑ä‡∂Ø‡∑í‡∂∫‡∑è‡∑Ä‡∑ö ‡∂∏‡∑è‡∂∫‡∑è‡∑Ä‡∂≠‡∑ì‡∂Ω‡∑è ‡∂∂‡∑î‡∂Ø‡∑î ‡∂Ø‡∑Ñ‡∂∏ ‡∑Ä‡∑ê‡∑Ö‡∂≥‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∑É‡∑ê‡∂ª‡∑É‡∑ô‡∂Ø‡∑ä‡∂Ø‡∑ì',\n",
              " '‡∂∏‡∑ö‡∂ö ‡∂á‡∂≠‡∑ä‡∂≠‡∂ß‡∂∏ ‡∂∏‡∑è‡∂±‡∑Ä ‡∂Ö‡∂∫‡∑í‡∂≠‡∑í‡∑Ä‡∑è‡∑É‡∑í‡∂ö‡∂∏‡∑ä ‡∂ã‡∂Ω‡∑ä‡∂Ω‡∂ú‡∂±‡∑ä‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ö‡∑ä',\n",
              " '‡∂ë‡∂ö‡∑ô‡∂ö‡∑ä ‡∂±‡∑Ñ‡∂∫‡∑ô ‡∂á‡∂ú‡∑í‡∂Ω‡∑ä‡∂Ω ‡∂ú‡∑Ñ‡∂Ω‡∑è ‡∂Ö‡∂±‡∑í‡∂ö‡∑è‡∂ß ‡∂¥‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑Ä‡∑è ‡∂∏‡∑ö ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è',\n",
              " '‡∂∏‡∑ö ‡∂±‡∑í‡∑É‡∑è ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂≠‡∑ù‡∂ª‡∑è ‡∂ú‡∂±‡∑ä‡∂±',\n",
              " '‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂¥‡∑í‡∂ª‡∑í‡∑É‡∑í‡∂≥‡∑î ‡∂ö‡∑ò‡∂≠‡∑í‡∂∏ ‡∂Ø‡∑í‡∂∫‡∂∏‡∂±‡∑ä‡∂≠‡∑í ‡∂â‡∂≠‡∑è ‡∂â‡∑Ñ‡∑Ö ‡∂≠‡∑è‡∂¥ ‡∑É‡∂±‡∑ä‡∂±‡∑è‡∂∫‡∂ö ‡∑Ä‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂±‡∑ú‡∑É‡∑ê‡∂Ω‡∂ö‡∑í‡∂∫‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂≠‡∂ª‡∂∏‡∑ä ‡∂ö‡∑î‡∂©‡∑è ‡∑Ä‡∑í‡∂Ø‡∑ä\\u200d‡∂∫‡∑î‡∂≠‡∑ä ‡∑É‡∂±‡∑ä‡∂±‡∑è‡∂∫‡∂ö‡∂≠‡∑è‡∑Ä‡∂ö‡∑ä ‡∂Ø‡∂ª‡∂∫‡∑í']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238ca5d3",
        "outputId": "104efb4e-94a3-4dfc-e736-e0eeec3e93de"
      },
      "source": [
        "len(lines)"
      ],
      "id": "238ca5d3",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15790635"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0afee9a2"
      },
      "source": [
        "def mlm(input_ids):\n",
        "    # input_ids = labels.detach().clone()\n",
        "    # create random array of floats with equal dims to input_ids\n",
        "    rand = torch.rand(input_ids.shape)\n",
        "    # mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "    mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
        "    # loop through each row in input_ids tensor (cannot do in parallel)\n",
        "    for i in range(input_ids.shape[0]):\n",
        "        # get indices of mask positions from mask array\n",
        "        # print(\"mask array shape:\", mask_arr.shape)\n",
        "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "        # print(\"selection array shape:\", selection)\n",
        "        # mask input_ids\n",
        "        input_ids[i, selection] = 4 \n",
        "    return input_ids"
      ],
      "id": "0afee9a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ecb82d"
      },
      "source": [
        "import torch\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        # store encodings internally\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of samples\n",
        "        return self.encodings['input_ids'].shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
        "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
      ],
      "id": "11ecb82d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2564316"
      },
      "source": [
        "# batch = tokenizer(lines[:100_000], max_length=512, padding='max_length', truncation=True)\n",
        "# len(batch)"
      ],
      "id": "b2564316",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68e29174"
      },
      "source": [
        "# # make copy of labels tensor, this will be input_ids\n",
        "# input_ids = labels.detach().clone()\n",
        "# # create random array of floats with equal dims to input_ids\n",
        "# rand = torch.rand(input_ids.shape)\n",
        "# # mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "# mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
        "# # loop through each row in input_ids tensor (cannot do in parallel)\n",
        "# for i in range(input_ids.shape[0]):\n",
        "#     # get indices of mask positions from mask array\n",
        "#     print(\"mask array shape:\", mask_arr.shape)\n",
        "#     selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "#     print(\"selection array shape:\", selection)\n",
        "#     # mask input_ids\n",
        "#     input_ids[i, selection] = 4  # our custom [MASK] token == 3"
      ],
      "id": "68e29174",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494cdd92"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,  # we align this to the tokenizer vocab_size\n",
        "    max_position_embeddings=514,\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=12,\n",
        "    type_vocab_size=1\n",
        ")"
      ],
      "id": "494cdd92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26b71d1"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config)"
      ],
      "id": "c26b71d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3101ba",
        "outputId": "1bf6edb3-2db2-41b9-ca87-3fc659c251d7"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)"
      ],
      "id": "af3101ba",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f863935e"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# activate training mode\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-4)"
      ],
      "id": "f863935e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39013ffa",
        "outputId": "5cd0b029-0d00-4a0b-9221-435fa61e61a4",
        "colab": {
          "referenced_widgets": [
            "edef190efc0c41c7a626bd213bc34e35"
          ]
        }
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "epochs = 1\n",
        "loss_values = []\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(lines), 5_00_000):\n",
        "        if i == (len(lines)//5_00_000)*5_00_000:\n",
        "            batch_data = tokenizer(lines[i:], max_length=512, padding='max_length', truncation=True)\n",
        "        else:\n",
        "            batch_data = tokenizer(lines[i:i+5_00_000], max_length=512, padding='max_length', truncation=True)\n",
        "        labels_all = torch.tensor(batch_data.input_ids)\n",
        "        mask_all = torch.tensor(batch_data.attention_mask)\n",
        "        input_ids_all = mlm(labels_all.detach().clone())\n",
        "        encodings = {'input_ids': input_ids_all, 'attention_mask': mask_all, 'labels': labels_all}\n",
        "        dataset = Dataset(encodings)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "    # setup loop with TQDM and dataloader\n",
        "        loop = tqdm(loader, leave=True)\n",
        "        for batch in loop:\n",
        "            # initialize calculated gradients (from prev step)\n",
        "            optim.zero_grad()\n",
        "            # pull all tensor batches required for training\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # process\n",
        "            outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                            labels=labels)\n",
        "            # extract loss\n",
        "            loss = outputs.loss\n",
        "            # calculate loss for every parameter that needs grad update\n",
        "            loss.backward()\n",
        "            # update parameters\n",
        "            optim.step()\n",
        "            # print relevant info to progress bar\n",
        "            loop.set_description(f'Epoch: {epoch} Text Loading Iter: {i//5_00_000}')\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "            loss_values.append(loss.item())\n",
        "            del input_ids\n",
        "            del attention_mask\n",
        "            del labels\n",
        "        else:\n",
        "            model.save_pretrained(f'./Roberta_models/roberta_model_{epoch}_{i}')\n",
        "            del batch_data\n",
        "            del labels_all \n",
        "            del mask_all \n",
        "            del input_ids_all\n",
        "            gc.collect()\n",
        "            with open('loss_value_file', 'wb') as fp:\n",
        "                pickle.dump(loss_values, fp)"
      ],
      "id": "39013ffa",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edef190efc0c41c7a626bd213bc34e35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/31250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1005481/1894666089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./Roberta_models/roberta_model_{epoch}_{i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#         if i%1_000_000 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/roberta_sinhala/rpt_env/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, save_config, state_dict, save_function, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# If we save using the predefined names, we can load using `from_pretrained`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0moutput_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model weights saved in {output_model_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/roberta_sinhala/rpt_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/roberta_sinhala/rpt_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77c0696a"
      },
      "source": [
        "# model.save_pretrained('./Test_on_the_way_bert')"
      ],
      "id": "77c0696a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f386e941"
      },
      "source": [
        "from transformers import pipeline\n",
        "fill = pipeline('fill-mask', model='Test_on_the_way_bert', tokenizer=tokenizer)"
      ],
      "id": "f386e941",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514dfcd1"
      },
      "source": [
        "fill(f'‡∂∏‡∂∏‡∂≠‡∑ä ‡∂ë‡∑Ñ‡∑ô‡∂∏ ‡∑Ä‡∑ô‡∂Ω‡∑è ‡∑Ñ‡∑í‡∂ß‡∑í‡∂∫ ‡∂±‡∑í‡∑É‡∑è {fill.tokenizer.mask_token} ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂Ø‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∂í‡∂Ø?')"
      ],
      "id": "514dfcd1",
      "execution_count": null,
      "outputs": []
    }
  ]
}